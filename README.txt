Instructions for Running the Code :

To see our results, please unzip the gru/gru_results.7z, rnn/rnn_results.7z and transformer/transformer_results.7z zip files.

####################################################################################################
####################################################################################################
For Q.4.1, and two settings of Q.4.2, the three experiments are carried out with running first 3 lines of the following files:
>> ./example_RNN.sh
>> ./example_GRU.sh

The hyper-parameter tuning experiments for RNN and GRU are carried out by running the rest of the lines of the following files:
>> ./example_RNN.sh
>> ./example_GRU.sh

For the transformer, simply run the experiments as shown in ptb-lm.py directly from this directory.

The folders that get created are supplied in the two directories: "rnn" and "gru". The best params are removed for keeping the size sane.
To get the learning curves, do the following:
>> cd rnn
>> ./plot_RNN.sh
(for RNN)

>> cd gru
>> ./plot_GRU.sh

...Please come back to the parent directory...

For the transformer, the results directories will be created in your current working directory.
Additionally, all transformer plots in the report can be generated by running the experiments.sh script in the transformer/ folder, followed by running all cells of the transformer/plot_transformer_curves.ipynb notebook.

####################################################################################################
####################################################################################################
For Q.5.1, for RNN, run--
>> python3 validation_loss_by_time.py --model=RNN --optimizer=ADAM --initial_lr=1e-4 --batch_size=128 --seq_len=35 --hidden_size=1500 --emb_size=256 --num_layers=2 --dp_keep_prob=0.5 --load=./
Then, for GRU, run--
>> python3 validation_loss_by_time.py --model=GRU --optimizer=SGD_LR_SCHEDULE --initial_lr=10 --batch_size=20 --seq_len=35 --hidden_size=1500 --emb_size=512 --num_layers=2 --dp_keep_prob=0.35 --load=./

For the transformer, cd into the transformer directory corresponding to the following settings:
--model=TRANSFORMER --optimizer=SGD --initial_lr=1 --batch_size=128 --seq_len=35 --hidden_size=256 --num_layers=12 --dp_keep_prob=0.9
and run the following script:
>> python3 validation_loss_by_time.py --model=GRU --optimizer=SGD --initial_lr=1 --batch_size=128 --seq_len=35 --hidden_size=256 --num_layers=12 --dp_keep_prob=0.9 --load=./

Then, to plot the results, run--
>> Plot_Loss.py

Here, we assumed that the hidden state should NOT be carried forward. To check the other scenario, where we DO carry the latent state, do--
>> python3 validation_loss_by_time_continual.py --model=RNN --optimizer=ADAM --initial_lr=1e-4 --batch_size=128 --seq_len=35 --hidden_size=1500 --emb_size=256 --num_layers=2 --dp_keep_prob=0.5 --load=./
, followed by
>> python3 validation_loss_by_time_continual.py --model=GRU --optimizer=SGD_LR_SCHEDULE --initial_lr=10 --batch_size=20 --seq_len=35 --hidden_size=1500 --emb_size=512 --num_layers=2 --dp_keep_prob=0.35 --load=./
, followed by
>> python3 Plot_Loss_Continual.py



####################################################################################################
####################################################################################################
For Q.5.2, for RNN, run--
>> python3 compute_gradients.py --model=RNN --initial_lr=1e-4 --batch_size=128 --seq_len=35 --hidden_size=1500 --emb_size=256 --num_layers=2 --dp_keep_prob=0.5 --load=./best_params_RNN.pt
Then, for GRU, run--
>> python3 compute_gradients.py --model=GRU --initial_lr=10 --batch_size=20 --seq_len=35 --hidden_size=1500 --emb_size=512 --num_layers=2 --dp_keep_prob=0.35 --load=./best_params_GRU.pt

Then, to plot the results, run--
>> python3 Plot_Gradients.py



####################################################################################################
####################################################################################################
For Q.5.3, for RNN, run--
>> python3 generate_samples.py --model=RNN --batch_size=10 --seq_len=35 --hidden_size=1500 --emb_size=256 --num_layers=2 --dp_keep_prob=0.5 --load=./best_params_RNN.pt
OR
>> python3 generate_samples.py --model=RNN --batch_size=10 --seq_len=70 --hidden_size=1500 --emb_size=256 --num_layers=2 --dp_keep_prob=0.5 --load=./best_params_RNN.pt
based on whether to see 35-long or 70 long sentences.

For GRU, run--
>> python3 generate_samples.py --model=GRU --batch_size=10 --seq_len=35 --hidden_size=1500 --emb_size=512 --num_layers=2 --dp_keep_prob=0.35 --load=./best_params_GRU.pt
OR
>> python3 generate_samples.py --model=GRU --batch_size=10 --seq_len=70 --hidden_size=1500 --emb_size=512 --num_layers=2 --dp_keep_prob=0.35 --load=./best_params_GRU.pt

Then, to see ALL the sentences, run--
>> python3 Get_Sentences.py